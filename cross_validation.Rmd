---
title: "cross_validation"
author: "Yingyu Cui"
date: "2024-11-12"
output: github_document
---
## for final proj, the data set could be ignored in the .gitignore file: "/data/file name" if it is too large in size

# cache is fine ?

##model selection under or over fitting bias or variance or generalization high bias and underfitting, high variance and overfitting, pridiction accuracy: training and testing --- split into 80/20 is fine and using RMSE to evaluate; repeat 1000 times or randomly choose and split for representative;  k 折交叉验证（Repeated k-fold Cross-Validation）; modelr package and tidy models package;


```{r}
library(modelr)
library(tidyverse)
library(mgcv)

install.packages("SemiPar")
library(SemiPar)

set.seed(1)

```
# operate the lidar data set 

# compare 3 models: linear, smooth, wiggly
sample_frac()--- sampling  size =  .8
anti_join() --- testing 
could add the testing data in geom_point()

# fit 3 models
lm()
mgcv::gam(df, s(), data = )
wiggly gam(df, s( k), sp data = )
and always use gam() for every thing

add_prediction in the table 
and and geom_line(pred) into the plot

recognize what is too fitting (too much curves) and what is too simple (too straight line)

and compare these models with RMSE
rmse(model, test_df) and compare the numbers

whether these variances between rmse are consistent --- repeat

# repeat training and testing 
crossv_mc() |> as_tibble()
use the mutate and map fuction for 
and extract rmse
anonymous function using \(x)
for rmse using map2_dbl

name_prefix()
ggplot on these rmse and geom_violin()

could change crossc_mc(test = 0.2) or something else, but most we use the 80/20 split

# example
weigh < 7 ---0 how this works 
pwl_model ?
and then same process as before for cv 

# Below is the code used in class
Load key packages.

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

# install.packages("SemiPar")
library(SemiPar)

set.seed(1)
```

look at LIDAR data

```{r}
data("lidar")

lidar_df =
  lidar |> 
  as_tibble() |> 
  mutate(id = row_number())
```

```{r}
lidar_df |> 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point()
```


## Try to do CV 

We'll compare 3 models -- one linear, one smooth, one wiggly.

Construct training and testing df

```{r}
train_df = sample_frac(lidar_df, size = .8)
test_df = anti_join(lidar_df, train_df, by = "id")
```

Look at these

```{r}
ggplot(train_df, aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")
```


Fit three models

```{r}
linear_mod = lm(logratio ~ range, data = train_df)
smooth_mod = gam(logratio ~ s(range), data = train_df)
wiggly_mod = gam(logratio ~ s(range, k = 30), sp = 10e-6, data = train_df)
```

Look at fits

```{r}
train_df |> 
  add_predictions(smooth_mod) |> 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point() + 
  # geom_point(data = test_df, color = "red") + 
  geom_line(aes(y = pred), color = "red")
```


Compare these numerically using RMSE.

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

## Repeat the train / test split

```{r}
cv_df = 
  crossv_mc(lidar_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```


```{r, eval = FALSE}
cv_df |> 
  pull(train) |> 
  nth(3) |> 
  as_tibble()
```


Fit models, extract RMSEs

```{r}
cv_res_df =
  cv_df |> 
  mutate(
    linear_mod = map(train, \(x) lm(logratio ~ range, data = x)),
    smooth_mod = map(train, \(x) gam(logratio ~ s(range), data = x)),
    wiggly_mod = map(train, \(x) gam(logratio ~ s(range, k = 30), sp = 10e-6, data = x))
  ) |> 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, rmse),
    rmse_smooth = map2_dbl(smooth_mod, test, rmse),
    rmse_wiggly = map2_dbl(wiggly_mod, test, rmse)
  )
```

Look at RMSE distribution


```{r}
cv_res_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```


## Nepalese children Df

```{r}
child_df = 
  read_csv("data/nepalese_children.csv") |> 
  mutate(
    weight_ch7 = (weight > 7) * (weight - 7)
  )
```

Look at data

```{r}
child_df |> 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5)
```

Fit some models

```{r}
linear_mod = lm(armc ~ weight, data = child_df)
pwl_mod    = lm(armc ~ weight + weight_ch7, data = child_df)
smooth_mod = gam(armc ~ s(weight), data = child_df)
```

Look at models

```{r}
child_df |> 
  add_predictions(smooth_mod) |> 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5) + 
  geom_line(aes(y = pred), color = "red")
```

CV to select models.

```{r}
cv_df = 
  crossv_mc(child_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Apply models and extract RMSE

```{r}
cv_res_df = 
  cv_df |> 
  mutate(
    linear_mod = map(train, \(x) lm(armc ~ weight, data = x)),
    pwl_mod    = map(train, \(x) lm(armc ~ weight + weight_ch7, data = x)),
    smooth_mod = map(train, \(x) gam(armc ~ s(weight), data = x)),
  ) |> 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, rmse),
    rmse_pwl    = map2_dbl(pwl_mod, test, rmse),
    rmse_smooth = map2_dbl(smooth_mod, test, rmse)
  )
```



```{r}
cv_res_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```



